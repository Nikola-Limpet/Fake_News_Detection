{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098509ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake News Detection System\n",
    "# Complete implementation for ITM-360 AI Class Project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Bidirectional, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# ======================== 1. DATA LOADING & PREPROCESSING ========================\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text data\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                 if token not in self.stop_words and len(token) > 2]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def load_and_preprocess_data(self, filepath):\n",
    "        \"\"\"Load and preprocess the dataset\"\"\"\n",
    "        # Load data\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.dropna(subset=['text', 'label'])\n",
    "        \n",
    "        # Clean text\n",
    "        df['cleaned_text'] = df['text'].apply(self.clean_text)\n",
    "        \n",
    "        # Encode labels (0: real, 1: fake)\n",
    "        le = LabelEncoder()\n",
    "        df['label_encoded'] = le.fit_transform(df['label'])\n",
    "        \n",
    "        return df, le\n",
    "\n",
    "# ======================== 2. FEATURE EXTRACTION ========================\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "        self.word2vec_model = None\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "    def extract_tfidf_features(self, texts_train, texts_test):\n",
    "        \"\"\"Extract TF-IDF features\"\"\"\n",
    "        X_train_tfidf = self.tfidf_vectorizer.fit_transform(texts_train)\n",
    "        X_test_tfidf = self.tfidf_vectorizer.transform(texts_test)\n",
    "        return X_train_tfidf, X_test_tfidf\n",
    "    \n",
    "    def train_word2vec(self, texts, vector_size=100, window=5, min_count=2):\n",
    "        \"\"\"Train Word2Vec model\"\"\"\n",
    "        tokenized_texts = [text.split() for text in texts]\n",
    "        self.word2vec_model = Word2Vec(tokenized_texts, vector_size=vector_size, \n",
    "                                       window=window, min_count=min_count, workers=4)\n",
    "        return self.word2vec_model\n",
    "    \n",
    "    def get_word2vec_features(self, texts):\n",
    "        \"\"\"Get Word2Vec features for texts\"\"\"\n",
    "        features = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            vec = np.zeros(self.word2vec_model.wv.vector_size)\n",
    "            count = 0\n",
    "            for word in words:\n",
    "                if word in self.word2vec_model.wv:\n",
    "                    vec += self.word2vec_model.wv[word]\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                vec = vec / count\n",
    "            features.append(vec)\n",
    "        return np.array(features)\n",
    "    \n",
    "    def get_bert_embeddings(self, texts, max_length=128, batch_size=32):\n",
    "        \"\"\"Extract BERT embeddings\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = self.bert_tokenizer.batch_encode_plus(\n",
    "                batch_texts,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Get BERT embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**encoded)\n",
    "                # Use [CLS] token embedding\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "\n",
    "# ======================== 3. MODEL TRAINING ========================\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def train_naive_bayes(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train Naive Bayes model\"\"\"\n",
    "        # Grid search for best parameters\n",
    "        param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}\n",
    "        nb_model = MultinomialNB()\n",
    "        grid_search = GridSearchCV(nb_model, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        self.models['naive_bayes'] = best_model\n",
    "        self.evaluate_model('Naive Bayes', y_test, y_pred)\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    def train_random_forest(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train Random Forest model\"\"\"\n",
    "        # Grid search for best parameters\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "        rf_model = RandomForestClassifier(random_state=42)\n",
    "        grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        self.models['random_forest'] = best_model\n",
    "        self.evaluate_model('Random Forest', y_test, y_pred)\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    def build_lstm_model(self, vocab_size, embedding_dim=100, max_length=100):\n",
    "        \"\"\"Build LSTM model\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "            Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_lstm(self, X_train, y_train, X_val, y_val, vocab_size, max_length=100):\n",
    "        \"\"\"Train LSTM model\"\"\"\n",
    "        model = self.build_lstm_model(vocab_size, max_length=max_length)\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "        checkpoint = ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
    "        reduce_lr = ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping, checkpoint, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.models['lstm'] = model\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    def build_bert_lstm_model(self, bert_dim=768, max_length=100):\n",
    "        \"\"\"Build BERT + LSTM hybrid model\"\"\"\n",
    "        # Input layer for BERT embeddings\n",
    "        bert_input = Input(shape=(bert_dim,), name='bert_input')\n",
    "        \n",
    "        # Dense layers for BERT features\n",
    "        bert_dense = Dense(256, activation='relu')(bert_input)\n",
    "        bert_dropout = Dropout(0.3)(bert_dense)\n",
    "        \n",
    "        # Additional dense layers\n",
    "        dense1 = Dense(128, activation='relu')(bert_dropout)\n",
    "        dropout1 = Dropout(0.3)(dense1)\n",
    "        \n",
    "        dense2 = Dense(64, activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(0.3)(dense2)\n",
    "        \n",
    "        # Output layer\n",
    "        output = Dense(1, activation='sigmoid')(dropout2)\n",
    "        \n",
    "        model = Model(inputs=bert_input, outputs=output)\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_bert_lstm(self, X_train_bert, y_train, X_val_bert, y_val):\n",
    "        \"\"\"Train BERT + LSTM hybrid model\"\"\"\n",
    "        model = self.build_bert_lstm_model()\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "        checkpoint = ModelCheckpoint('best_bert_lstm_model.h5', save_best_only=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train_bert, y_train,\n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_data=(X_val_bert, y_val),\n",
    "            callbacks=[early_stopping, checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.models['bert_lstm'] = model\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    def evaluate_model(self, model_name, y_true, y_pred):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        self.results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        return self.results[model_name]\n",
    "\n",
    "# ======================== 4. EXPLAINABILITY ========================\n",
    "\n",
    "class ModelExplainer:\n",
    "    def __init__(self, model, vectorizer=None):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def explain_with_lime(self, text, num_features=10):\n",
    "        \"\"\"Explain prediction using LIME\"\"\"\n",
    "        explainer = LimeTextExplainer(class_names=['Real', 'Fake'])\n",
    "        \n",
    "        def predict_proba(texts):\n",
    "            if self.vectorizer:\n",
    "                X = self.vectorizer.transform(texts)\n",
    "                return self.model.predict_proba(X)\n",
    "            else:\n",
    "                # For neural networks\n",
    "                return self.model.predict(texts)\n",
    "        \n",
    "        exp = explainer.explain_instance(text, predict_proba, num_features=num_features)\n",
    "        \n",
    "        return exp\n",
    "    \n",
    "    def explain_with_shap(self, X_train_sample, X_test_sample):\n",
    "        \"\"\"Explain predictions using SHAP\"\"\"\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            # For sklearn models\n",
    "            explainer = shap.Explainer(self.model.predict_proba, X_train_sample)\n",
    "            shap_values = explainer(X_test_sample)\n",
    "        else:\n",
    "            # For neural networks\n",
    "            explainer = shap.DeepExplainer(self.model, X_train_sample)\n",
    "            shap_values = explainer.shap_values(X_test_sample)\n",
    "        \n",
    "        return shap_values\n",
    "\n",
    "# ======================== 5. VISUALIZATION ========================\n",
    "\n",
    "class Visualizer:\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Real', 'Fake'], \n",
    "                   yticklabels=['Real', 'Fake'])\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_roc_curve(y_true, y_scores, title='ROC Curve'):\n",
    "        \"\"\"Plot ROC curve\"\"\"\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "        roc_auc = roc_auc_score(y_true, y_scores)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(title)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_training_history(history):\n",
    "        \"\"\"Plot training history for neural networks\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        axes[0].set_title('Model Accuracy')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Loss\n",
    "        axes[1].plot(history.history['loss'], label='Training Loss')\n",
    "        axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "        axes[1].set_title('Model Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_word_clouds(real_texts, fake_texts):\n",
    "        \"\"\"Plot word clouds for real and fake news\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Real news word cloud\n",
    "        real_wordcloud = WordCloud(width=800, height=400, \n",
    "                                   background_color='white').generate(' '.join(real_texts))\n",
    "        axes[0].imshow(real_wordcloud, interpolation='bilinear')\n",
    "        axes[0].set_title('Real News Word Cloud')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Fake news word cloud\n",
    "        fake_wordcloud = WordCloud(width=800, height=400, \n",
    "                                   background_color='white').generate(' '.join(fake_texts))\n",
    "        axes[1].imshow(fake_wordcloud, interpolation='bilinear')\n",
    "        axes[1].set_title('Fake News Word Cloud')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ======================== 6. MAIN PIPELINE ========================\n",
    "\n",
    "class FakeNewsDetectionPipeline:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = DataPreprocessor()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.model_trainer = ModelTrainer()\n",
    "        self.visualizer = Visualizer()\n",
    "        \n",
    "    def run_pipeline(self, data_path):\n",
    "        \"\"\"Run the complete fake news detection pipeline\"\"\"\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"FAKE NEWS DETECTION SYSTEM\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # 1. Load and preprocess data\n",
    "        print(\"\\n1. Loading and preprocessing data...\")\n",
    "        df, label_encoder = self.preprocessor.load_and_preprocess_data(data_path)\n",
    "        print(f\"Data loaded: {len(df)} samples\")\n",
    "        print(f\"Class distribution:\\n{df['label'].value_counts()}\")\n",
    "        \n",
    "        # 2. Split data\n",
    "        print(\"\\n2. Splitting data...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['cleaned_text'], df['label_encoded'], \n",
    "            test_size=0.2, random_state=42, stratify=df['label_encoded']\n",
    "        )\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, \n",
    "            test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {len(X_train)} samples\")\n",
    "        print(f\"Validation set: {len(X_val)} samples\")\n",
    "        print(f\"Test set: {len(X_test)} samples\")\n",
    "        \n",
    "        # 3. Feature extraction\n",
    "        print(\"\\n3. Extracting features...\")\n",
    "        \n",
    "        # TF-IDF features\n",
    "        print(\"   - Extracting TF-IDF features...\")\n",
    "        X_train_tfidf, X_test_tfidf = self.feature_extractor.extract_tfidf_features(\n",
    "            X_train, X_test\n",
    "        )\n",
    "        X_val_tfidf = self.feature_extractor.tfidf_vectorizer.transform(X_val)\n",
    "        \n",
    "        # Word2Vec features\n",
    "        print(\"   - Training Word2Vec model...\")\n",
    "        self.feature_extractor.train_word2vec(X_train)\n",
    "        X_train_w2v = self.feature_extractor.get_word2vec_features(X_train)\n",
    "        X_val_w2v = self.feature_extractor.get_word2vec_features(X_val)\n",
    "        X_test_w2v = self.feature_extractor.get_word2vec_features(X_test)\n",
    "        \n",
    "        # BERT embeddings (using smaller subset for demo)\n",
    "        print(\"   - Extracting BERT embeddings (this may take a while)...\")\n",
    "        # Note: For full implementation, process all data\n",
    "        # Here we use a subset for demonstration\n",
    "        sample_size = min(1000, len(X_train))\n",
    "        X_train_bert = self.feature_extractor.get_bert_embeddings(\n",
    "            X_train[:sample_size].tolist()\n",
    "        )\n",
    "        X_val_bert = self.feature_extractor.get_bert_embeddings(\n",
    "            X_val[:min(200, len(X_val))].tolist()\n",
    "        )\n",
    "        X_test_bert = self.feature_extractor.get_bert_embeddings(\n",
    "            X_test[:min(200, len(X_test))].tolist()\n",
    "        )\n",
    "        \n",
    "        # 4. Train models\n",
    "        print(\"\\n4. Training models...\")\n",
    "        \n",
    "        # Baseline ML models\n",
    "        print(\"\\n   a) Training Naive Bayes...\")\n",
    "        nb_model = self.model_trainer.train_naive_bayes(\n",
    "            X_train_tfidf, y_train, X_test_tfidf, y_test\n",
    "        )\n",
    "        \n",
    "        print(\"\\n   b) Training Random Forest...\")\n",
    "        rf_model = self.model_trainer.train_random_forest(\n",
    "            X_train_w2v, y_train, X_test_w2v, y_test\n",
    "        )\n",
    "        \n",
    "        # Deep Learning models\n",
    "        print(\"\\n   c) Training LSTM...\")\n",
    "        # Prepare data for LSTM\n",
    "        tokenizer = Tokenizer(num_words=5000)\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        \n",
    "        X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "        X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "        X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "        \n",
    "        max_length = 100\n",
    "        X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "        X_val_pad = pad_sequences(X_val_seq, maxlen=max_length)\n",
    "        X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "        \n",
    "        lstm_model, lstm_history = self.model_trainer.train_lstm(\n",
    "            X_train_pad, y_train, X_val_pad, y_val, \n",
    "            vocab_size=5000, max_length=max_length\n",
    "        )\n",
    "        \n",
    "        # Evaluate LSTM\n",
    "        lstm_pred = (lstm_model.predict(X_test_pad) > 0.5).astype(int).flatten()\n",
    "        self.model_trainer.evaluate_model('LSTM', y_test, lstm_pred)\n",
    "        \n",
    "        print(\"\\n   d) Training BERT + Dense Network...\")\n",
    "        bert_model, bert_history = self.model_trainer.train_bert_lstm(\n",
    "            X_train_bert, y_train[:sample_size], \n",
    "            X_val_bert, y_val[:min(200, len(y_val))]\n",
    "        )\n",
    "        \n",
    "        # Evaluate BERT model\n",
    "        bert_pred = (bert_model.predict(X_test_bert) > 0.5).astype(int).flatten()\n",
    "        self.model_trainer.evaluate_model(\n",
    "            'BERT + Dense', y_test[:min(200, len(y_test))], bert_pred\n",
    "        )\n",
    "        \n",
    "        # 5. Generate visualizations\n",
    "        print(\"\\n5. Generating visualizations...\")\n",
    "        \n",
    "        # Confusion matrices\n",
    "        self.visualizer.plot_confusion_matrix(y_test, nb_model.predict(X_test_tfidf), \n",
    "                                             'Naive Bayes Confusion Matrix')\n",
    "        \n",
    "        # ROC curves\n",
    "        if hasattr(nb_model, 'predict_proba'):\n",
    "            nb_scores = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "            self.visualizer.plot_roc_curve(y_test, nb_scores, 'Naive Bayes ROC Curve')\n",
    "        \n",
    "        # Training history for neural networks\n",
    "        self.visualizer.plot_training_history(lstm_history)\n",
    "        \n",
    "        # Word clouds\n",
    "        real_texts = df[df['label_encoded'] == 0]['cleaned_text'].tolist()[:500]\n",
    "        fake_texts = df[df['label_encoded'] == 1]['cleaned_text'].tolist()[:500]\n",
    "        self.visualizer.plot_word_clouds(real_texts, fake_texts)\n",
    "        \n",
    "        # 6. Model explainability\n",
    "        print(\"\\n6. Generating model explanations...\")\n",
    "        \n",
    "        # LIME explanation for a sample\n",
    "        sample_text = X_test.iloc[0]\n",
    "        explainer = ModelExplainer(nb_model, self.feature_extractor.tfidf_vectorizer)\n",
    "        lime_exp = explainer.explain_with_lime(sample_text)\n",
    "        print(\"\\nLIME Explanation for sample text:\")\n",
    "        print(lime_exp.as_list()[:5])  # Top 5 features\n",
    "        \n",
    "        # 7. Save models\n",
    "        print(\"\\n7. Saving models...\")\n",
    "        \n",
    "        # Save preprocessor\n",
    "        with open('preprocessor.pkl', 'wb') as f:\n",
    "            pickle.dump(self.preprocessor, f)\n",
    "        \n",
    "        # Save feature extractors\n",
    "        with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.feature_extractor.tfidf_vectorizer, f)\n",
    "        \n",
    "        # Save ML models\n",
    "        with open('naive_bayes_model.pkl', 'wb') as f:\n",
    "            pickle.dump(nb_model, f)\n",
    "        \n",
    "        with open('random_forest_model.pkl', 'wb') as f:\n",
    "            pickle.dump(rf_model, f)\n",
    "        \n",
    "        # Save tokenizer for LSTM\n",
    "        with open('lstm_tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "        \n",
    "        print(\"\\nModels saved successfully!\")\n",
    "        \n",
    "        # 8. Print final results summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"FINAL RESULTS SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        results_df = pd.DataFrame(self.model_trainer.results).T\n",
    "        print(results_df.to_string())\n",
    "        \n",
    "        return self.model_trainer.models, self.model_trainer.results\n",
    "\n",
    "# ======================== 7. USAGE EXAMPLE ========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = FakeNewsDetectionPipeline()\n",
    "    \n",
    "    # Run the pipeline with your dataset\n",
    "    # Replace 'fake_news_data.csv' with your actual dataset path\n",
    "    # The dataset should have 'text' and 'label' columns\n",
    "    \n",
    "    try:\n",
    "        models, results = pipeline.run_pipeline('fake_news_data.csv')\n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nNote: Please ensure you have the fake news dataset.\")\n",
    "        print(\"You can download it from Kaggle:\")\n",
    "        print(\"https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\")\n",
    "        print(\"\\nExpected CSV format:\")\n",
    "        print(\"- 'text': The news article text\")\n",
    "        print(\"- 'label': 'REAL' or 'FAKE'\")\n",
    "        \n",
    "        # Create a small sample dataset for demonstration\n",
    "        print(\"\\nCreating sample dataset for demonstration...\")\n",
    "        sample_data = pd.DataFrame({\n",
    "            'text': [\n",
    "                \"The president announced new economic policies today...\",\n",
    "                \"Scientists discover miracle cure that doctors hate...\",\n",
    "                \"Stock market reaches new highs amid strong earnings...\",\n",
    "                \"Aliens confirmed to be living among us, government admits...\",\n",
    "                \"Climate change study shows concerning trends...\",\n",
    "                \"Celebrity secretly a lizard person, insider reveals...\"\n",
    "            ],\n",
    "            'label': ['REAL', 'FAKE', 'REAL', 'FAKE', 'REAL', 'FAKE']\n",
    "        })\n",
    "        sample_data.to_csv('sample_fake_news_data.csv', index=False)\n",
    "        print(\"Sample dataset created: 'sample_fake_news_data.csv'\")\n",
    "        \n",
    "        # Run with sample data\n",
    "        models, results = pipeline.run_pipeline('sample_fake_news_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
